{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 6616,
     "status": "ok",
     "timestamp": 1651210830226,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "VmDIQPYx5Vwq"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24442,
     "status": "ok",
     "timestamp": 1651210854642,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "nuCrYTX2b0Qt",
    "outputId": "cb0678a4-c951-46e4-dc7e-edfda505cd4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "g8jR6llzbxUE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXeXpWevbxUF"
   },
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1651210856016,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "KOACv4C2bxUI"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "sequence_length = 64\n",
    "\n",
    "path = '.'\n",
    "engFile = 'sentencesTrain.txt'\n",
    "signFile = 'tokensTrain.txt'\n",
    "with codecs.open(path + '/' + engFile, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    engSamples = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "with codecs.open(path + '/' + signFile, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    signSamples = f.read().split(\"\\n\")[:-1]\n",
    "tokenToInx = {}\n",
    "inxToToken = {}\n",
    "curInx = 1\n",
    "signInx = []\n",
    "signMask = []\n",
    "for sent in signSamples:\n",
    "    tokens = ['[START]'] + sent.split(',') + ['[END]']\n",
    "    inxes = []\n",
    "    tmpMask = []\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if(token not in tokenToInx.keys()):\n",
    "            tokenToInx[token] = curInx\n",
    "            inxToToken[curInx] = token\n",
    "            curInx += 1\n",
    "        inxes.append(tokenToInx[token])  \n",
    "        tmpMask.append(1)\n",
    "    for i in range(len(inxes), sequence_length):\n",
    "        inxes.append(0)\n",
    "        tmpMask.append(0)\n",
    "    signInx.append(inxes)\n",
    "    signMask.append(tmpMask)\n",
    "signVocabSize = len(tokenToInx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651210856017,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "KsW5by4abxUJ"
   },
   "outputs": [],
   "source": [
    "tokenToInx['[PAD]'] = 0\n",
    "inxToToken[0] = '[PAD]'\n",
    "STARTINX = tokenToInx['[START]']\n",
    "ENDINX = tokenToInx['[END]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1651210856964,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "44JKmMc6bxUK"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,standardize=custom_standardization\n",
    ")\n",
    "\n",
    "eng_vectorization.adapt(engSamples)\n",
    "engInx = eng_vectorization(engSamples)\n",
    "engMask = tf.cast(engInx != 0, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1651210860838,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "7VEz2nyKbxUN"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(signInx)):\n",
    "    data.append((engInx[i], engMask[i], signInx[i], signMask[i]))\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651210861393,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "58p4ptt4bxUO"
   },
   "outputs": [],
   "source": [
    "engInx = [sample[0] for sample in data]\n",
    "engMask = [sample[1] for sample in data]\n",
    "signInx = [sample[2] for sample in data]\n",
    "signMask = [sample[3] for sample in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1651210862534,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "-Eny7zNjbxUP"
   },
   "outputs": [],
   "source": [
    "signInx = tf.convert_to_tensor(signInx)\n",
    "signMask = tf.convert_to_tensor(signMask)\n",
    "engInx = tf.convert_to_tensor(engInx)\n",
    "engMask = tf.convert_to_tensor(engMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651210863427,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "7fvx9u3fbxUQ"
   },
   "outputs": [],
   "source": [
    "size = len(data)\n",
    "val_ratio = 0.15\n",
    "val_size = int(val_ratio * size)\n",
    "train_size = size - val_size\n",
    "\n",
    "train_zeros = tf.cast(tf.zeros((train_size,1)),tf.int32)\n",
    "val_zeros = tf.cast(tf.zeros((size - train_size,1)), tf.int32)\n",
    "train_engInx = engInx[:train_size]\n",
    "train_engMask = engMask[:train_size]\n",
    "train_signInx = signInx[:train_size]\n",
    "train_signMask = signMask[:train_size]\n",
    "y_train = tf.concat((train_signInx[:,1:], train_zeros), axis=1)\n",
    "\n",
    "val_engInx = engInx[train_size:]\n",
    "val_engMask = engMask[train_size:]\n",
    "val_signInx = signInx[train_size:]\n",
    "val_signMask = signMask[train_size:]\n",
    "y_val = tf.concat((val_signInx[:,1:], val_zeros), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzBVgjfLbxUT"
   },
   "source": [
    "# Implement Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1651210866138,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "Bd-KP4jg5ZH9"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 1741,
     "status": "ok",
     "timestamp": 1651226629911,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "6zRc_OOwbxUV"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "num_heads = 4\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "encoder_masks = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_masks\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "#encoder1 = TransformerEncoder(embed_dim, latent_dim, num_heads)(x, encoder_masks)\n",
    "#dropout1 = layers.Dropout(0.4)(encoder1)\n",
    "#encoder2 = TransformerEncoder(embed_dim, latent_dim, num_heads)(dropout1, encoder_masks)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x, encoder_masks)\n",
    "encoder = keras.Model([encoder_inputs,encoder_masks], encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "decoder_masks = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_masks\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "#x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs, decoder_masks)\n",
    "#x = layers.Dropout(0.4)(x)\n",
    "#x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs, decoder_masks)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs, decoder_masks)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(signVocabSize+1, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs, decoder_masks], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs,decoder_masks])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, encoder_masks, decoder_inputs, decoder_masks], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1651226634165,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "lBWGGJ6EbxUW",
    "outputId": "c1df0d0a-642d-48b2-b676-84de3722e45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_10 (Posit  (None, None, 256)   3856384     ['encoder_inputs[0][0]']         \n",
      " ionalEmbedding)                                                                                  \n",
      "                                                                                                  \n",
      " encoder_masks (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_7 (Transfo  (None, None, 256)   1578496     ['positional_embedding_10[0][0]',\n",
      " rmerEncoder)                                                     'encoder_masks[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_masks (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " model_11 (Functional)          (None, None, 1222)   6801350     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_7[0][0]',  \n",
      "                                                                  'decoder_masks[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,236,230\n",
      "Trainable params: 12,236,230\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3050480,
     "status": "error",
     "timestamp": 1651229687808,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "lJcWL6dVbxUW",
    "outputId": "6aaca7e9-2d80-4a2f-977e-4b27279a423f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 28s 1s/step - loss: 1.2496 - accuracy: 0.1450 - val_loss: 1.1335 - val_accuracy: 0.2043\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.0543 - accuracy: 0.2158 - val_loss: 1.0873 - val_accuracy: 0.2221\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 23s 1s/step - loss: 0.9043 - accuracy: 0.2722 - val_loss: 1.0292 - val_accuracy: 0.2607\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 22s 1s/step - loss: 0.7498 - accuracy: 0.3772 - val_loss: 0.9881 - val_accuracy: 0.2786\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 20s 1s/step - loss: 0.6060 - accuracy: 0.4891 - val_loss: 0.9536 - val_accuracy: 0.3029\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 23s 1s/step - loss: 0.4871 - accuracy: 0.5786 - val_loss: 0.9356 - val_accuracy: 0.3221\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 22s 1s/step - loss: 0.3922 - accuracy: 0.6528 - val_loss: 0.9154 - val_accuracy: 0.3379\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 21s 1s/step - loss: 0.3184 - accuracy: 0.7120 - val_loss: 0.9095 - val_accuracy: 0.3436\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 22s 1s/step - loss: 0.2559 - accuracy: 0.7636 - val_loss: 0.9199 - val_accuracy: 0.3529\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 24s 1s/step - loss: 0.2097 - accuracy: 0.8084 - val_loss: 0.9151 - val_accuracy: 0.3493\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 21s 1s/step - loss: 0.1680 - accuracy: 0.8502 - val_loss: 0.9207 - val_accuracy: 0.3543\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 21s 1s/step - loss: 0.1373 - accuracy: 0.8782 - val_loss: 0.9343 - val_accuracy: 0.3614\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 24s 1s/step - loss: 0.1074 - accuracy: 0.9043 - val_loss: 0.9417 - val_accuracy: 0.3686\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 22s 1s/step - loss: 0.0891 - accuracy: 0.9222 - val_loss: 0.9475 - val_accuracy: 0.3686\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 20s 1s/step - loss: 0.0736 - accuracy: 0.9362 - val_loss: 0.9551 - val_accuracy: 0.3671\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 22s 1s/step - loss: 0.0626 - accuracy: 0.9439 - val_loss: 0.9768 - val_accuracy: 0.3593\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 22s 1s/step - loss: 0.0550 - accuracy: 0.9507 - val_loss: 0.9745 - val_accuracy: 0.3686\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 21s 1s/step - loss: 0.0486 - accuracy: 0.9575 - val_loss: 0.9959 - val_accuracy: 0.3714\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 20s 1s/step - loss: 0.0422 - accuracy: 0.9624 - val_loss: 0.9997 - val_accuracy: 0.3693\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 24s 1s/step - loss: 0.0379 - accuracy: 0.9649 - val_loss: 1.0086 - val_accuracy: 0.3707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f884c052100>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit((train_engInx, train_engMask, train_signInx, train_signMask), y_train, epochs=epochs, validation_data=((val_engInx, val_engMask, val_signInx, val_signMask), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1651229697901,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "kL2kSdIQbxUX"
   },
   "outputs": [],
   "source": [
    "transformer.save_weights(path + \"/transformer_\" + str(embed_dim) + \"_\" + str(latent_dim) + \"_\" + str(num_heads) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1651187452113,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "VE7ZfmcBcNaC"
   },
   "outputs": [],
   "source": [
    "transformer.load_weights(path + \"/transformer_\" + str(embed_dim) + \"_\" + str(latent_dim) + \"_\" + str(num_heads) + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Kq1nmqVdPMV"
   },
   "source": [
    "#Evaluate Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1651187660888,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "IEWKro2WbxUX"
   },
   "outputs": [],
   "source": [
    "max_decoded_sentence_length = 64\n",
    "\n",
    "def createMask(input):\n",
    "    return tf.cast(input != 0, tf.int64)\n",
    "def padding(input):\n",
    "    ans = []\n",
    "    for tokens in input:\n",
    "        tmp = []\n",
    "        for token in tokens:\n",
    "            tmp.append(token)\n",
    "        for i in range(len(tokens), max_decoded_sentence_length):\n",
    "            tmp.append(0)\n",
    "        ans.append(tmp)\n",
    "    return tf.convert_to_tensor(ans)\n",
    "def decode_sequence(tokenized_input_sentence):\n",
    "    tokenized_target_sentence = [[1]]\n",
    "    tokensTarget = \"\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        paddedTarget = padding(tokenized_target_sentence)\n",
    "        maskTarget = createMask(paddedTarget)\n",
    "        maskEng = createMask(tokenized_input_sentence)\n",
    "        predictions = transformer([tokenized_input_sentence, maskEng,paddedTarget, maskTarget])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = inxToToken[sampled_token_index]\n",
    "        if sampled_token == \"[END]\":\n",
    "            break\n",
    "        tokenized_target_sentence[0].append(sampled_token_index)\n",
    "        if(i != 0):\n",
    "            tokensTarget += ','\n",
    "        tokensTarget += sampled_token\n",
    "    return tokenized_target_sentence, tokensTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135855,
     "status": "ok",
     "timestamp": 1651187798118,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "y1RLd2crbxUY",
    "outputId": "2e766041-2304-4526-c347-91098bbff141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 of 106"
     ]
    }
   ],
   "source": [
    "testFile = 'sentencesTest.txt'\n",
    "with codecs.open(path + '/' + testFile, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    engTest = f.read().split(\"\\n\")[:-1]\n",
    "ans = []\n",
    "for inx,sentence in enumerate(engTest):\n",
    "    print(f'\\r{inx+1} of {len(engTest)}', end='')\n",
    "    tokens = eng_vectorization([tf.strings.lower(sentence)])\n",
    "    inxTarget, tokensTarget = decode_sequence(tokens)\n",
    "    ans.append((sentence, tokensTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1651188066861,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "es3TCBEndOUX"
   },
   "outputs": [],
   "source": [
    "results = [tuple[1] for tuple in ans]\n",
    "file = codecs.open(path + '/' + \"tokensTest.txt\", \"w\", \"utf-8\")\n",
    "file.write(\"\\n\".join(results))\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
