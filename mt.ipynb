{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6616,
     "status": "ok",
     "timestamp": 1651210830226,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "VmDIQPYx5Vwq"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24442,
     "status": "ok",
     "timestamp": 1651210854642,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "nuCrYTX2b0Qt",
    "outputId": "cb0678a4-c951-46e4-dc7e-edfda505cd4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8jR6llzbxUE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXeXpWevbxUF"
   },
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1651210856016,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "KOACv4C2bxUI"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "sequence_length = 64\n",
    "\n",
    "path = '.'\n",
    "engFile = 'sentencesTrain.txt'\n",
    "signFile = 'tokensTrain.txt'\n",
    "with codecs.open(path + '/' + engFile, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    engSamples = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "with codecs.open(path + '/' + signFile, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    signSamples = f.read().split(\"\\n\")[:-1]\n",
    "tokenToInx = {}\n",
    "inxToToken = {}\n",
    "curInx = 1\n",
    "signInx = []\n",
    "signMask = []\n",
    "for sent in signSamples:\n",
    "    tokens = ['[START]'] + sent.split(',') + ['[END]']\n",
    "    inxes = []\n",
    "    tmpMask = []\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if(token not in tokenToInx.keys()):\n",
    "            tokenToInx[token] = curInx\n",
    "            inxToToken[curInx] = token\n",
    "            curInx += 1\n",
    "        inxes.append(tokenToInx[token])  \n",
    "        tmpMask.append(1)\n",
    "    for i in range(len(inxes), sequence_length):\n",
    "        inxes.append(0)\n",
    "        tmpMask.append(0)\n",
    "    signInx.append(inxes)\n",
    "    signMask.append(tmpMask)\n",
    "signVocabSize = len(tokenToInx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651210856017,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "KsW5by4abxUJ"
   },
   "outputs": [],
   "source": [
    "tokenToInx['[PAD]'] = 0\n",
    "inxToToken[0] = '[PAD]'\n",
    "STARTINX = tokenToInx['[START]']\n",
    "ENDINX = tokenToInx['[END]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1651210856964,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "44JKmMc6bxUK"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,standardize=custom_standardization\n",
    ")\n",
    "\n",
    "eng_vectorization.adapt(engSamples)\n",
    "engInx = eng_vectorization(engSamples)\n",
    "engMask = tf.cast(engInx != 0, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1651210860838,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "7VEz2nyKbxUN"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(signInx)):\n",
    "    data.append((engInx[i], engMask[i], signInx[i], signMask[i]))\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1651210861393,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "58p4ptt4bxUO"
   },
   "outputs": [],
   "source": [
    "engInx = [sample[0] for sample in data]\n",
    "engMask = [sample[1] for sample in data]\n",
    "signInx = [sample[2] for sample in data]\n",
    "signMask = [sample[3] for sample in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1651210862534,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "-Eny7zNjbxUP"
   },
   "outputs": [],
   "source": [
    "signInx = tf.convert_to_tensor(signInx)\n",
    "signMask = tf.convert_to_tensor(signMask)\n",
    "engInx = tf.convert_to_tensor(engInx)\n",
    "engMask = tf.convert_to_tensor(engMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651210863427,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "7fvx9u3fbxUQ"
   },
   "outputs": [],
   "source": [
    "size = len(data)\n",
    "val_ratio = 0.1\n",
    "val_size = int(val_ratio * size)\n",
    "train_size = size - val_size\n",
    "\n",
    "train_zeros = tf.cast(tf.zeros((train_size,1)),tf.int32)\n",
    "val_zeros = tf.cast(tf.zeros((size - train_size,1)), tf.int32)\n",
    "train_engInx = engInx[:train_size]\n",
    "train_engMask = engMask[:train_size]\n",
    "train_signInx = signInx[:train_size]\n",
    "train_signMask = signMask[:train_size]\n",
    "y_train = tf.concat((train_signInx[:,1:], train_zeros), axis=1)\n",
    "\n",
    "val_engInx = engInx[train_size:]\n",
    "val_engMask = engMask[train_size:]\n",
    "val_signInx = signInx[train_size:]\n",
    "val_signMask = signMask[train_size:]\n",
    "y_val = tf.concat((val_signInx[:,1:], val_zeros), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzBVgjfLbxUT"
   },
   "source": [
    "# Implement Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1651210866138,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "Bd-KP4jg5ZH9"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1741,
     "status": "ok",
     "timestamp": 1651226629911,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "6zRc_OOwbxUV"
   },
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "encoder_masks = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_masks\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "#encoder1 = TransformerEncoder(embed_dim, latent_dim, num_heads)(x, encoder_masks)\n",
    "#dropout1 = layers.Dropout(0.4)(encoder1)\n",
    "#encoder2 = TransformerEncoder(embed_dim, latent_dim, num_heads)(dropout1, encoder_masks)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x, encoder_masks)\n",
    "encoder = keras.Model([encoder_inputs,encoder_masks], encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "decoder_masks = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_masks\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "#x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs, decoder_masks)\n",
    "#x = layers.Dropout(0.4)(x)\n",
    "#x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs, decoder_masks)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs, decoder_masks)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(signVocabSize+1, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs, decoder_masks], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs,decoder_masks])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, encoder_masks, decoder_inputs, decoder_masks], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1651226634165,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "lBWGGJ6EbxUW",
    "outputId": "c1df0d0a-642d-48b2-b676-84de3722e45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_7 (Positi  (None, None, 512)   7712768     ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " encoder_masks (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_8 (Transfo  (None, None, 512)   10503168    ['positional_embedding_7[0][0]', \n",
      " rmerEncoder)                                                     'encoder_masks[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_masks (InputLayer)     [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " model_7 (Functional)           (None, None, 1222)   27245254    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_8[0][0]',  \n",
      "                                                                  'decoder_masks[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 45,461,190\n",
      "Trainable params: 45,461,190\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 30  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3050480,
     "status": "error",
     "timestamp": 1651229687808,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "lJcWL6dVbxUW",
    "outputId": "6aaca7e9-2d80-4a2f-977e-4b27279a423f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 156s 8s/step - loss: 1.2675 - accuracy: 0.1344 - val_loss: 1.2393 - val_accuracy: 0.1827\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 151s 8s/step - loss: 1.0977 - accuracy: 0.2000 - val_loss: 1.2000 - val_accuracy: 0.1980\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 153s 8s/step - loss: 0.9539 - accuracy: 0.2385 - val_loss: 1.1176 - val_accuracy: 0.2245\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.7411 - accuracy: 0.3515 - val_loss: 1.0323 - val_accuracy: 0.2918\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.5337 - accuracy: 0.5034 - val_loss: 1.0095 - val_accuracy: 0.3010\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 151s 8s/step - loss: 0.3882 - accuracy: 0.6147 - val_loss: 0.9712 - val_accuracy: 0.3327\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.2855 - accuracy: 0.7022 - val_loss: 0.9630 - val_accuracy: 0.3582\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 163s 8s/step - loss: 0.2146 - accuracy: 0.7681 - val_loss: 0.9704 - val_accuracy: 0.3602\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 151s 8s/step - loss: 0.1672 - accuracy: 0.8158 - val_loss: 0.9578 - val_accuracy: 0.3796\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 151s 8s/step - loss: 0.1410 - accuracy: 0.8442 - val_loss: 0.9745 - val_accuracy: 0.3694\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.1234 - accuracy: 0.8592 - val_loss: 0.9893 - val_accuracy: 0.3745\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.1017 - accuracy: 0.8860 - val_loss: 1.0108 - val_accuracy: 0.3847\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.0838 - accuracy: 0.9032 - val_loss: 1.0090 - val_accuracy: 0.3796\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.0734 - accuracy: 0.9172 - val_loss: 1.0099 - val_accuracy: 0.3918\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 151s 8s/step - loss: 0.0655 - accuracy: 0.9236 - val_loss: 1.0445 - val_accuracy: 0.3837\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 150s 7s/step - loss: 0.0581 - accuracy: 0.9360 - val_loss: 1.0295 - val_accuracy: 0.3949\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 149s 7s/step - loss: 0.0517 - accuracy: 0.9386 - val_loss: 1.0508 - val_accuracy: 0.3684\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 150s 7s/step - loss: 0.0465 - accuracy: 0.9467 - val_loss: 1.0780 - val_accuracy: 0.3724\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.0428 - accuracy: 0.9502 - val_loss: 1.0726 - val_accuracy: 0.3867\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 150s 8s/step - loss: 0.0408 - accuracy: 0.9529 - val_loss: 1.0499 - val_accuracy: 0.3765\n",
      "Epoch 21/30\n",
      " 2/20 [==>...........................] - ETA: 2:12 - loss: 0.0349 - accuracy: 0.9587"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6e3c2703f09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_engInx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_engMask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_signInx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_signMask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_engInx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_engMask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_signInx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_signMask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.fit((train_engInx, train_engMask, train_signInx, train_signMask), y_train, epochs=epochs, validation_data=((val_engInx, val_engMask, val_signInx, val_signMask), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1651229697901,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "kL2kSdIQbxUX"
   },
   "outputs": [],
   "source": [
    "transformer.save_weights(\"transformer_512.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1651187452113,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "VE7ZfmcBcNaC"
   },
   "outputs": [],
   "source": [
    "transformer.load_weights(path + \"/transformer.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Kq1nmqVdPMV"
   },
   "source": [
    "#Evaluate Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1651187660888,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "IEWKro2WbxUX"
   },
   "outputs": [],
   "source": [
    "max_decoded_sentence_length = 64\n",
    "\n",
    "def createMask(input):\n",
    "    return tf.cast(input != 0, tf.int64)\n",
    "def padding(input):\n",
    "    ans = []\n",
    "    for tokens in input:\n",
    "        tmp = []\n",
    "        for token in tokens:\n",
    "            tmp.append(token)\n",
    "        for i in range(len(tokens), max_decoded_sentence_length):\n",
    "            tmp.append(0)\n",
    "        ans.append(tmp)\n",
    "    return tf.convert_to_tensor(ans)\n",
    "def decode_sequence(tokenized_input_sentence):\n",
    "    tokenized_target_sentence = [[1]]\n",
    "    tokensTarget = \"\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        paddedTarget = padding(tokenized_target_sentence)\n",
    "        maskTarget = createMask(paddedTarget)\n",
    "        maskEng = createMask(tokenized_input_sentence)\n",
    "        predictions = transformer([tokenized_input_sentence, maskEng,paddedTarget, maskTarget])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = inxToToken[sampled_token_index]\n",
    "        if sampled_token == \"[END]\":\n",
    "            break\n",
    "        tokenized_target_sentence[0].append(sampled_token_index)\n",
    "        if(i != 0):\n",
    "            tokensTarget += ','\n",
    "        tokensTarget += sampled_token\n",
    "    return tokenized_target_sentence, tokensTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135855,
     "status": "ok",
     "timestamp": 1651187798118,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "y1RLd2crbxUY",
    "outputId": "2e766041-2304-4526-c347-91098bbff141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 of 106"
     ]
    }
   ],
   "source": [
    "testFile = 'sentencesTest.txt'\n",
    "with codecs.open(path + '/' + testFile, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    engTest = f.read().split(\"\\n\")[:-1]\n",
    "ans = []\n",
    "for inx,sentence in enumerate(engTest):\n",
    "    print(f'\\r{inx+1} of {len(engTest)}', end='')\n",
    "    tokens = eng_vectorization([tf.strings.lower(sentence)])\n",
    "    inxTarget, tokensTarget = decode_sequence(tokens)\n",
    "    ans.append((sentence, tokensTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1651188066861,
     "user": {
      "displayName": "korn sooksatra",
      "userId": "06817929792724594815"
     },
     "user_tz": 300
    },
    "id": "es3TCBEndOUX"
   },
   "outputs": [],
   "source": [
    "results = [tuple[1] for tuple in ans]\n",
    "file = codecs.open(path + '/' + \"tokensTest.txt\", \"w\", \"utf-8\")\n",
    "file.write(\"\\n\".join(results))\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
